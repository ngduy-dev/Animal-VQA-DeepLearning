{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10993681,"sourceType":"datasetVersion","datasetId":6843027},{"sourceId":11028595,"sourceType":"datasetVersion","datasetId":6868230},{"sourceId":11047719,"sourceType":"datasetVersion","datasetId":6882137},{"sourceId":11075661,"sourceType":"datasetVersion","datasetId":6902674},{"sourceId":11076109,"sourceType":"datasetVersion","datasetId":6903025},{"sourceId":11076615,"sourceType":"datasetVersion","datasetId":6903400}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# X√¢y d·ª±ng Class VQADataset ƒë·ªÉ load Dataset","metadata":{"id":"E6QNJsnIPL5y"}},{"cell_type":"code","source":"import os\nfrom torchvision import transforms\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nimport pickle\nimport pandas as pd\nfrom tqdm import tqdm\nfrom collections import Counter\nimport ast\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom torchvision.models import resnet50, ResNet50_Weights\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"id":"_7RJf-XDPL55","outputId":"79565709-9c27-4d5a-d202-30df624eb8b5","execution":{"iopub.status.busy":"2025-03-26T02:56:10.594503Z","iopub.execute_input":"2025-03-26T02:56:10.594962Z","iopub.status.idle":"2025-03-26T02:56:17.841073Z","shell.execute_reply.started":"2025-03-26T02:56:10.594912Z","shell.execute_reply":"2025-03-26T02:56:17.840245Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**T·∫°o c√°c h·∫±ng s·ªë l∆∞u input**","metadata":{"id":"hgt0sgXpPL58"}},{"cell_type":"code","source":"# ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch√≠nh\n# DATASET_PATH = \"/kaggle/working/midterm_deep_learning/dataset\"\n\n# T·∫°o c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt\n# IMAGES_PATH = os.path.join(DATASET_PATH, \"images\")\nIMAGES_PATH = os.path.join(\"/kaggle/input/imagesaa/images\")\n\n# Load dataset vqa\n# df_vqa = pd.read_csv(os.path.join(DATASET_PATH, \"dataset_vqa.csv\"))\ndf_vqa = pd.read_csv(\"/kaggle/input/dataset-ver2/dataset_vqa.csv\")","metadata":{"trusted":true,"id":"G49d4EozPL59","execution":{"iopub.status.busy":"2025-03-26T02:56:17.842022Z","iopub.execute_input":"2025-03-26T02:56:17.842466Z","iopub.status.idle":"2025-03-26T02:56:17.895383Z","shell.execute_reply.started":"2025-03-26T02:56:17.842442Z","shell.execute_reply":"2025-03-26T02:56:17.894741Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import shutil\n\nshutil.copy('/kaggle/input/dataset-ver2/dataset_vqa.csv', '/kaggle/working/')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:56:17.897106Z","iopub.execute_input":"2025-03-26T02:56:17.897412Z","iopub.status.idle":"2025-03-26T02:56:17.904738Z","shell.execute_reply.started":"2025-03-26T02:56:17.897384Z","shell.execute_reply":"2025-03-26T02:56:17.903990Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/dataset_vqa.csv'"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# Ti·ªÅn x·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o (resize, chuy·ªÉn v·ªÅ tensor, ...)","metadata":{"id":"sd_WXh6BGGUj"}},{"cell_type":"code","source":"# L·∫•y danh s√°ch c√°c ID ·∫£nh\nimage_id_list = df_vqa[\"image_id\"].unique().tolist()\n\n# ƒê·ªãnh nghƒ©a transform ƒë·ªÉ chu·∫©n h√≥a ·∫£nh\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize ·∫£nh v·ªÅ 224x224\n    transforms.ToTensor(),  # Chuy·ªÉn ·∫£nh th√†nh tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Chu·∫©n h√≥a theo ImageNet\n])\n\n# Dictionary ƒë·ªÉ l∆∞u tensor c·ªßa ·∫£nh\nimage_tensors = {}\nimage_id_tensors = []\n\n# X·ª≠ l√Ω t·ª´ng ·∫£nh v√† theo d√µi ti·∫øn ƒë·ªô\nfor image_id in tqdm(image_id_list, desc=\"ƒêang x·ª≠ l√Ω ·∫£nh\"):\n    image_path = os.path.join(IMAGES_PATH, f\"{image_id}.jpg\")\n\n    if os.path.exists(image_path):  # Ki·ªÉm tra ·∫£nh c√≥ t·ªìn t·∫°i kh√¥ng\n        img = Image.open(image_path).convert(\"RGB\")  # M·ªü ·∫£nh\n        img_tensor = transform(img)  # √Åp d·ª•ng transform\n        image_tensors[image_id] = img_tensor  # L∆∞u tensor ·∫£nh\n        image_id_tensors.append(image_id)  # L∆∞u ID ·∫£nh v√†o danh s√°ch\n    else:\n        print(f\"C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y ·∫£nh {image_id}\")\n\n# Chuy·ªÉn danh s√°ch ID ·∫£nh th√†nh tensor PyTorch\nimage_id_tensors = torch.tensor(image_id_tensors)\n\n# Ki·ªÉm tra s·ªë l∆∞·ª£ng ·∫£nh ƒë√£ tensor h√≥a\nprint(f\"Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω {len(image_tensors)} / {len(image_id_list)} ·∫£nh.\")\n","metadata":{"id":"0OwhIFKUSWn4","outputId":"cf928ffb-0d97-47fb-e606-40585d37b5d2","trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:56:17.905837Z","iopub.execute_input":"2025-03-26T02:56:17.906103Z","iopub.status.idle":"2025-03-26T02:57:40.076860Z","shell.execute_reply.started":"2025-03-26T02:56:17.906084Z","shell.execute_reply":"2025-03-26T02:57:40.075901Z"}},"outputs":[{"name":"stderr","text":"ƒêang x·ª≠ l√Ω ·∫£nh: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8599/8599 [01:22<00:00, 104.68it/s]","output_type":"stream"},{"name":"stdout","text":"Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω 8599 / 8599 ·∫£nh.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# X√¢y d·ª±ng b·ªô vocab cho qu√° tr√¨nh vectorize c√¢u h·ªèi","metadata":{"id":"jGOc2ucMIzzO"}},{"cell_type":"code","source":"def build_vocab(dataset, min_freq=1):\n    word_counter = Counter()\n\n    # Tokenize c√¢u h·ªèi\n    for question in dataset[\"question\"]:\n        words = word_tokenize(question)  # D√πng word_tokenize ƒë·ªÉ t√°ch t·ª´\n        word_counter.update(words)\n\n    # L·ªçc t·ª´ c√≥ t·∫ßn su·∫•t >= `min_freq`\n    vocab = [\"<PAD>\", \"<UNK>\", \"<START>\", \"<END>\"] + [word for word, count in word_counter.items() if count >= min_freq]\n\n    # T·∫°o √°nh x·∫° t·ª´ ‚Üí ID v√† ng∆∞·ª£c l·∫°i\n    word2idx = {word: idx for idx, word in enumerate(vocab)}\n    idx2word = {idx: word for word, idx in word2idx.items()}\n\n    return word2idx, idx2word\n\n# X√¢y d·ª±ng vocab t·ª´ df_vqa\nword2idx, idx2word = build_vocab(df_vqa, min_freq=1)\n\n# Ki·ªÉm tra k√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn\nprint(f\"S·ªë l∆∞·ª£ng t·ª´ trong vocab: {len(word2idx)}\")","metadata":{"id":"rloqlfE3HSRe","trusted":true,"outputId":"bb9401b9-630f-4555-aa56-00b573101efe","execution":{"iopub.status.busy":"2025-03-26T02:57:40.077795Z","iopub.execute_input":"2025-03-26T02:57:40.078112Z","iopub.status.idle":"2025-03-26T02:57:41.088063Z","shell.execute_reply.started":"2025-03-26T02:57:40.078080Z","shell.execute_reply":"2025-03-26T02:57:41.087181Z"}},"outputs":[{"name":"stdout","text":"S·ªë l∆∞·ª£ng t·ª´ trong vocab: 1457\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Vectorize cho c√¢u h·ªèi","metadata":{"id":"QmWKh38Cfzod"}},{"cell_type":"code","source":"MAX_QUESTION_LENGTH = 22  # ƒê·ªãnh nghƒ©a ƒë·ªô d√†i t·ªëi ƒëa c·ªßa c√¢u h·ªèi\n\ndef vectorize_question(question, word2idx, max_length=MAX_QUESTION_LENGTH):\n    tokens = word_tokenize(question)  # T√°ch t·ª´\n    vectorized = [word2idx.get(word, word2idx[\"<UNK>\"]) for word in tokens]  # Chuy·ªÉn th√†nh ID\n\n    # Th√™m padding n·∫øu c√¢u ng·∫Øn h∆°n max_length\n    if len(vectorized) < max_length:\n        vectorized += [word2idx[\"<PAD>\"]] * (max_length - len(vectorized))\n    else:\n        vectorized = vectorized[:max_length]  # C·∫Øt b·ªõt n·∫øu qu√° d√†i\n\n    return vectorized\n\nmax_length = 20  # ƒê·ªãnh nghƒ©a ƒë·ªô d√†i t·ªëi ƒëa c·ªßa c√¢u h·ªèi\n\ndf_vqa[\"question_vector\"] = df_vqa[\"question\"].apply(lambda x: vectorize_question(x, word2idx, max_length))\n","metadata":{"trusted":true,"id":"shad9mrMfzod","execution":{"iopub.status.busy":"2025-03-26T02:57:41.088800Z","iopub.execute_input":"2025-03-26T02:57:41.089006Z","iopub.status.idle":"2025-03-26T02:57:42.057658Z","shell.execute_reply.started":"2025-03-26T02:57:41.088988Z","shell.execute_reply":"2025-03-26T02:57:42.056785Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"print(df_vqa.head())\n# df_vqa.to_csv('/kaggle/working/midterm_deep_learning/dataset/dataset_vqa.csv', index=False)\ndf_vqa.to_csv('/kaggle/working/dataset_vqa.csv', index=False)","metadata":{"trusted":true,"id":"MFBK0V6rfzod","outputId":"3e5061d2-e7af-4e41-b51e-5e055cbbe61d","execution":{"iopub.status.busy":"2025-03-26T02:57:42.058512Z","iopub.execute_input":"2025-03-26T02:57:42.058870Z","iopub.status.idle":"2025-03-26T02:57:42.150098Z","shell.execute_reply.started":"2025-03-26T02:57:42.058840Z","shell.execute_reply":"2025-03-26T02:57:42.149227Z"}},"outputs":[{"name":"stdout","text":"   image_id                             question multiple_choice_answer  \\\n0    137045                         what is this                   bear   \n1    131093             how many sheeps are this                      3   \n2        25          how many giraffes are there                      2   \n3        25   how many animals are in this photo                      2   \n4    370986  how many elephants are in the water                      2   \n\n                                              answer  label  \\\n0  ['bear', 'bear', 'bear', 'bear', 'bear', 'bear...     28   \n1  ['3', '3', '3', '3', '3', '0', '3', '3', '3', ...    202   \n2  ['2', '2', '2', '1', '2', '1', '2', '2', '2', ...    216   \n3  ['2', '2', 'i can see total of 2 animals', '2'...    216   \n4  ['2', '2', '1', '2', '2', '2', '3', '2', '2', ...    216   \n\n                                     question_vector  \n0  [4, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n1  [7, 8, 9, 10, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n2  [7, 8, 11, 10, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n3  [7, 8, 13, 10, 14, 6, 15, 0, 0, 0, 0, 0, 0, 0,...  \n4  [7, 8, 16, 10, 14, 17, 18, 0, 0, 0, 0, 0, 0, 0...  \n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# X√¢y d·ª±ng l·ªõp VQADataset ƒë·ªÉ kh·ªüi t·∫°o dataset cho CNN v√† LSTM","metadata":{"id":"3OzwuQaYfzoe"}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport os\nimport torch\n\nclass VQADataset(Dataset):\n    def __init__(self, df, image_tensors):\n        self.df = df\n        self.image_tensors = image_tensors  # Dictionary ch·ª©a tensor ·∫£nh\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image_id = self.df.iloc[idx][\"image_id\"]\n        question_vector = torch.tensor(self.df.iloc[idx][\"question_vector\"], dtype=torch.long)\n        label = torch.tensor(self.df.iloc[idx][\"label\"], dtype=torch.long)\n\n        # L·∫•y tensor ·∫£nh t·ª´ dictionary\n        image_tensor = self.image_tensors.get(image_id, torch.zeros(3, 224, 224))  # D·ª± ph√≤ng ·∫£nh kh√¥ng t·ªìn t·∫°i\n\n        return image_id, image_tensor, question_vector, label\n\n\n# H√†m l·ªçc d·ªØ li·ªáu b·ªã None tr∆∞·ªõc khi ƒë∆∞a v√†o DataLoader\ndef collate_fn(batch):\n    batch = [b for b in batch if b is not None]  # Lo·∫°i b·ªè None\n    if len(batch) == 0:\n        return None\n    return torch.utils.data.default_collate(batch)\n\n# T·∫°o dataset & dataloader v·ªõi ·∫£nh ƒë√£ tensor h√≥a\ndataset = VQADataset(df_vqa, image_tensors)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=collate_fn)\n","metadata":{"trusted":true,"id":"4pKQEj00fzoe","execution":{"iopub.status.busy":"2025-03-26T02:57:42.152595Z","iopub.execute_input":"2025-03-26T02:57:42.152865Z","iopub.status.idle":"2025-03-26T02:57:42.159394Z","shell.execute_reply.started":"2025-03-26T02:57:42.152845Z","shell.execute_reply":"2025-03-26T02:57:42.158472Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# X√¢y d·ª±ng model ImageFeatureExtractor s·ª≠ d·ª•ng pre-trained model ResNet50 ƒë·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng ·∫£nh","metadata":{"id":"LWCyIZJnfzoe"}},{"cell_type":"markdown","source":"**X√¢y d·ª±ng ImageFeatureExtractor**","metadata":{"id":"aDzMPQhqfzoe"}},{"cell_type":"code","source":"class ResNetFeatureExtractor(nn.Module):\n    def __init__(self, output_dim=2048):\n        super(ResNetFeatureExtractor, self).__init__()\n        resnet = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n\n        # ƒê√≥ng bƒÉng c√°c tham s·ªë\n        for param in resnet.parameters():\n            param.requires_grad = False\n\n        # Lo·∫°i b·ªè FC cu·ªëi c√πng\n        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n\n        # ƒê∆∞a v·ªÅ k√≠ch th∆∞·ªõc output_dim mong mu·ªën\n        self.fc = nn.Linear(2048, output_dim) if output_dim != 2048 else nn.Identity()\n\n    def forward(self, images):\n        with torch.no_grad():\n            features = self.feature_extractor(images)  # [batch, 2048, 1, 1]\n\n        features = features.view(features.size(0), -1)  # [batch, 2048]\n        return self.fc(features)  # [batch, output_dim]\n","metadata":{"trusted":true,"id":"4y4OfLHYfzoe","execution":{"iopub.status.busy":"2025-03-26T03:47:02.911341Z","iopub.execute_input":"2025-03-26T03:47:02.911710Z","iopub.status.idle":"2025-03-26T03:47:02.917458Z","shell.execute_reply.started":"2025-03-26T03:47:02.911665Z","shell.execute_reply":"2025-03-26T03:47:02.916625Z"}},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":"# X√¢y d·ª±ng model QuestionLSTM ƒë·ªÉ sinh output","metadata":{"id":"SB4MAUvwfzoe"}},{"cell_type":"markdown","source":"Load Pre-trained Embeddings v√†o nn.Embedding","metadata":{}},{"cell_type":"code","source":"def load_glove_embeddings(glove_path, word_to_idx, embedding_dim=100):\n    embeddings_index = {}\n\n    # Load GloVe file\n    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vector = np.asarray(values[1:], dtype=\"float32\")\n            embeddings_index[word] = vector\n\n    # Initialize embedding matrix\n    vocab_size = len(word_to_idx)\n    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n\n    for word, idx in word_to_idx.items():\n        if word in embeddings_index:\n            embedding_matrix[idx] = embeddings_index[word]\n        else:\n            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))  # Random init for unknown words\n\n    return torch.tensor(embedding_matrix, dtype=torch.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T03:28:58.267926Z","iopub.execute_input":"2025-03-26T03:28:58.268226Z","iopub.status.idle":"2025-03-26T03:28:58.273573Z","shell.execute_reply.started":"2025-03-26T03:28:58.268201Z","shell.execute_reply":"2025-03-26T03:28:58.272765Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"class QuestionLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=512, num_layers=2, pretrained_embeddings=None):\n        super(QuestionLSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n\n        # Word Embedding\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        if pretrained_embeddings is not None:\n            self.embedding.weight = nn.Parameter(pretrained_embeddings)\n            self.embedding.weight.requires_grad = False  # Freeze embeddings\n        \n        # LSTM Encoder\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=False, dropout=0.3)\n\n    def forward(self, questions):\n        embedded = self.embedding(questions)  # [batch, seq_len, embedding_dim]\n        lstm_out, (h_n, c_n) = self.lstm(embedded)  # lstm_out: [batch, seq_len, hidden_dim]\n\n        question_vector = h_n[-1]  # L·∫•y hidden state cu·ªëi c√πng [batch, hidden_dim]\n        return question_vector  # [batch, hidden_dim]\n","metadata":{"trusted":true,"id":"fleA6lWEfzoe","execution":{"iopub.status.busy":"2025-03-26T03:47:08.751495Z","iopub.execute_input":"2025-03-26T03:47:08.751845Z","iopub.status.idle":"2025-03-26T03:47:08.757601Z","shell.execute_reply.started":"2025-03-26T03:47:08.751815Z","shell.execute_reply":"2025-03-26T03:47:08.756795Z"}},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":"# X√¢y d·ª±ng class VQAModel ho√†n ch·ªânh (CNN + LSTM)","metadata":{"id":"AHkEAZ40fzoe"}},{"cell_type":"code","source":"class VQAModel(nn.Module):\n    def __init__(self, resnet_extractor, lstm_encoder, vocab_size, embedding_dim, hidden_dim, num_layers=2, sos_token=1):\n        super().__init__()\n        self.resnet_extractor = resnet_extractor  # Tr√≠ch xu·∫•t ·∫£nh\n        self.lstm_encoder = lstm_encoder  # X·ª≠ l√Ω c√¢u h·ªèi\n        self.sos_token = sos_token  # Token b·∫Øt ƒë·∫ßu\n\n        self.embedding_layer = nn.Embedding(vocab_size, hidden_dim)  # Embedding cho decoder input\n\n        # Decoder LSTM (ƒë·∫ßu v√†o c√≥ k√≠ch th∆∞·ªõc 2048 + hidden_dim)\n        self.decoder = nn.LSTM(hidden_dim + 2048, hidden_dim, num_layers, batch_first=True)\n        self.fc_out = nn.Linear(hidden_dim, vocab_size)  # D·ª± ƒëo√°n token ti·∫øp theo\n\n    def forward(self, images, questions, target_seq_length):\n        # T√°ch ri√™ng ƒë·∫∑c tr∆∞ng ·∫£nh v√† c√¢u h·ªèi ƒë·ªÉ tr√°nh n·ªëi th·ª´a\n        image_features = self.resnet_extractor(images)  # [batch, 2048]\n        question_features = self.lstm_encoder(questions)  # [batch, hidden_dim]\n        \n        # Gi·ªØ nguy√™n k√≠ch th∆∞·ªõc image_features l√† 2048\n        context_vector = image_features.unsqueeze(1)  # [batch, 1, 2048]\n        \n        batch_size = images.size(0)\n        hidden = (torch.zeros(self.decoder.num_layers, batch_size, self.decoder.hidden_size).to(images.device),\n                  torch.zeros(self.decoder.num_layers, batch_size, self.decoder.hidden_size).to(images.device))\n        \n        decoder_input = torch.full((batch_size, 1), self.sos_token, dtype=torch.long).to(images.device)\n        \n        outputs = []\n        for _ in range(target_seq_length):\n            embedded_input = self.embedding_layer(decoder_input)  # [batch, 1, hidden_dim]\n        \n            # üî• **S·ª≠a l·ªói: ch·ªâ n·ªëi `image_features` thay v√¨ `context_vector`**\n            decoder_input = torch.cat((image_features.unsqueeze(1), embedded_input), dim=-1)  # [batch, 1, 2048 + hidden_dim]\n        \n            decoder_output, hidden = self.decoder(decoder_input, hidden)  # [batch, 1, hidden_dim]\n            token_logits = self.fc_out(decoder_output)  # [batch, 1, vocab_size]\n        \n            outputs.append(token_logits)\n            decoder_input = token_logits.argmax(dim=-1)  # [batch, 1]\n        \n        return torch.cat(outputs, dim=1)  # [batch, target_seq_length, vocab_size]\n\n\n        \n","metadata":{"trusted":true,"id":"33Xzlhctfzoe","execution":{"iopub.status.busy":"2025-03-26T03:51:53.129371Z","iopub.execute_input":"2025-03-26T03:51:53.129685Z","iopub.status.idle":"2025-03-26T03:51:53.137679Z","shell.execute_reply.started":"2025-03-26T03:51:53.129661Z","shell.execute_reply":"2025-03-26T03:51:53.136937Z"}},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":"# Chia t·∫≠p train, val","metadata":{"id":"ZE2wPGnCfzof"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Chia train (70%), val (30%)\ndf_train, df_val = train_test_split(df_vqa, test_size=0.3, random_state=42)\n\nprint(f\"Train: {len(df_train)}, Val: {len(df_val)}\")\n\n# T·∫°o dataset\ntrain_dataset = VQADataset(df_train, image_tensors)\nval_dataset = VQADataset(df_val, image_tensors)\n\n# T·∫°o DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n","metadata":{"trusted":true,"id":"PFv813ysfzof","outputId":"bcd75b13-16dc-41bb-b282-4f01dd61ede4","execution":{"iopub.status.busy":"2025-03-26T03:34:29.521569Z","iopub.execute_input":"2025-03-26T03:34:29.521913Z","iopub.status.idle":"2025-03-26T03:34:29.533777Z","shell.execute_reply.started":"2025-03-26T03:34:29.521875Z","shell.execute_reply":"2025-03-26T03:34:29.532961Z"}},"outputs":[{"name":"stdout","text":"Train: 9598, Val: 4114\n","output_type":"stream"}],"execution_count":55},{"cell_type":"markdown","source":"# Hu·∫•n luy·ªán m√¥ h√¨nh","metadata":{"id":"KR-YrC3ffzof"}},{"cell_type":"code","source":"# Ki·ªÉm tra thi·∫øt b·ªã\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# Kh·ªüi t·∫°o ResNet50 tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng ·∫£nh\nresnet_extractor = ResNetFeatureExtractor(output_dim=2048)\n\n# Load pre-trained GloVe embeddings\nglove_path = \"/kaggle/input/dataset-300mb-100d/glove.6B.100d.txt\"  # C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n n·∫øu c·∫ßn\npretrained_embeddings = load_glove_embeddings(glove_path, word2idx)\n\n# Kh·ªüi t·∫°o LSTM Encoder v·ªõi GloVe\nvocab_size = len(word2idx) + 1  # K√≠ch th∆∞·ªõc t·ª´ v·ª±ng (+1 v√¨ c√≥ token PAD ho·∫∑c UNK)\nuse_attention = True\n# lstm_encoder = QuestionLSTM(vocab_size, pretrained_embeddings=pretrained_embeddings, use_attention=use_attention)\n\n# Kh·ªüi t·∫°o VQAModel (Seq2Seq)\nembedding_dim = 100  # K√≠ch th∆∞·ªõc embedding (GloVe 100d)\nhidden_dim = 512  # Hidden size c·ªßa LSTM\ntarget_seq_length = 10  # ƒê·ªô d√†i t·ªëi ƒëa c·ªßa c√¢u tr·∫£ l·ªùi\n\nlstm_encoder = QuestionLSTM(vocab_size, embedding_dim=100, hidden_dim=512)\n\nvqa_model = VQAModel(resnet_extractor, lstm_encoder, vocab_size, embedding_dim, hidden_dim).to(device)\n# Loss function & Optimizer (Cho Seq2Seq)\ncriterion = nn.CrossEntropyLoss(ignore_index=0)  # B·ªè qua token PAD n·∫øu c√≥\noptimizer = optim.Adam(vqa_model.parameters(), lr=1e-4, weight_decay=1e-5)  # Th√™m weight decay ƒë·ªÉ tr√°nh overfitting","metadata":{"trusted":true,"id":"FM1Fm8W2fzof","outputId":"ef61ba56-8995-4d24-ac8d-f8b8a80fa33b","execution":{"iopub.status.busy":"2025-03-26T04:03:09.149132Z","iopub.execute_input":"2025-03-26T04:03:09.149559Z","iopub.status.idle":"2025-03-26T04:03:17.925402Z","shell.execute_reply.started":"2025-03-26T04:03:09.149518Z","shell.execute_reply":"2025-03-26T04:03:17.924746Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\n# Danh s√°ch l∆∞u l·ªãch s·ª≠ loss\ntrain_losses = []\nval_losses = []\n\nnum_epochs = 50\nbest_val_loss = float('inf')  # L∆∞u loss th·∫•p nh·∫•t (v√¨ v·ªõi Seq2Seq, loss quan tr·ªçng h∆°n accuracy)\n\nfor epoch in range(num_epochs):\n    # ----- TRAINING -----\n    vqa_model.train()\n    total_loss = 0\n\n    print(f\"\\nEpoch {epoch+1}/{num_epochs} - Training...\")\n\n    for batch_idx, (image_ids, images, questions, answers) in enumerate(train_loader):\n        images, questions, answers = images.to(device), questions.to(device), answers.to(device)\n\n        optimizer.zero_grad()\n        answers = answers.unsqueeze(1)\n        # print(answers.shape)\n        \n        outputs = vqa_model(images, questions, target_seq_length=answers.shape[1])  # [batch, seq_len, vocab_size]\n        \n        loss = criterion(outputs.view(-1, outputs.size(-1)), answers.view(-1))  # Flatten ƒë·ªÉ t√≠nh CrossEntropyLoss\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        # Hi·ªÉn th·ªã sau m·ªói 50 step\n        if (batch_idx + 1) % 50 == 0 or batch_idx == len(train_loader) - 1:\n            print(f\"Step [{batch_idx+1}/{len(train_loader)}] - Loss: {loss.item():.4f}\")\n\n    train_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} DONE - Train Loss: {train_loss:.4f}\")\n    \n    # L∆∞u train loss v√†o l·ªãch s·ª≠\n    train_losses.append(train_loss)\n\n    # ----- VALIDATION -----\n    vqa_model.eval()\n    val_loss = 0\n    num_batches = 0  \n\n    with torch.no_grad():  # Kh√¥ng t√≠nh gradient khi validation\n        for batch_idx, (image_ids, images, questions, answers) in enumerate(val_loader):\n            images, questions, answers = images.to(device), questions.to(device), answers.to(device)\n            answers = answers.unsqueeze(1)\n            outputs = vqa_model(images, questions, target_seq_length=answers.shape[1])\n            \n            \n            loss = criterion(outputs.view(-1, outputs.size(-1)), answers.view(-1))\n\n            val_loss += loss.item()\n            num_batches += 1\n\n    avg_val_loss = val_loss / num_batches if num_batches > 0 else float('inf')\n\n    print(f\"Validation - Loss: {avg_val_loss:.4f}\")\n\n    # L∆∞u validation loss v√†o l·ªãch s·ª≠\n    val_losses.append(avg_val_loss)\n\n    # ----- L∆∞u model t·ªët nh·∫•t d·ª±a tr√™n validation loss -----\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        model_filename = \"vqa_seq2seq_best.pth\"\n\n        # L∆∞u model\n        torch.save(vqa_model.state_dict(), model_filename)\n        print(f\"Best model updated! Saved as {model_filename}\")\n\n# V·∫Ω bi·ªÉu ƒë·ªì loss\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\nplt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T04:03:27.118059Z","iopub.execute_input":"2025-03-26T04:03:27.118343Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/50 - Training...\nStep [50/300] - Loss: 3.6950\nStep [100/300] - Loss: 2.8802\nStep [150/300] - Loss: 2.9243\nStep [200/300] - Loss: 3.4546\nStep [250/300] - Loss: 3.1869\nStep [300/300] - Loss: 3.5913\nEpoch 1 DONE - Train Loss: 3.7529\nValidation - Loss: 3.2045\nBest model updated! Saved as vqa_seq2seq_best.pth\n\nEpoch 2/50 - Training...\nStep [50/300] - Loss: 2.8488\nStep [100/300] - Loss: 3.3041\nStep [150/300] - Loss: 3.1308\nStep [200/300] - Loss: 2.9575\nStep [250/300] - Loss: 2.4570\nStep [300/300] - Loss: 3.2893\nEpoch 2 DONE - Train Loss: 3.1909\nValidation - Loss: 3.2096\n\nEpoch 3/50 - Training...\nStep [50/300] - Loss: 3.4800\nStep [100/300] - Loss: 3.9182\nStep [150/300] - Loss: 3.5510\nStep [200/300] - Loss: 2.8914\nStep [250/300] - Loss: 3.4659\nStep [300/300] - Loss: 2.9380\nEpoch 3 DONE - Train Loss: 3.1522\nValidation - Loss: 3.1844\nBest model updated! Saved as vqa_seq2seq_best.pth\n\nEpoch 4/50 - Training...\nStep [50/300] - Loss: 2.4814\nStep [100/300] - Loss: 3.7412\nStep [150/300] - Loss: 2.9570\nStep [200/300] - Loss: 3.3693\nStep [250/300] - Loss: 2.6630\nStep [300/300] - Loss: 2.9383\nEpoch 4 DONE - Train Loss: 3.0917\nValidation - Loss: 3.1183\nBest model updated! Saved as vqa_seq2seq_best.pth\n\nEpoch 5/50 - Training...\nStep [50/300] - Loss: 3.2338\nStep [100/300] - Loss: 3.0801\nStep [150/300] - Loss: 2.9706\nStep [200/300] - Loss: 2.6884\nStep [250/300] - Loss: 3.4095\nStep [300/300] - Loss: 3.5929\nEpoch 5 DONE - Train Loss: 3.0323\nValidation - Loss: 3.0966\nBest model updated! Saved as vqa_seq2seq_best.pth\n\nEpoch 6/50 - Training...\nStep [50/300] - Loss: 2.6317\nStep [100/300] - Loss: 3.0736\nStep [150/300] - Loss: 3.1450\nStep [200/300] - Loss: 2.6229\nStep [250/300] - Loss: 2.7203\nStep [300/300] - Loss: 4.1092\nEpoch 6 DONE - Train Loss: 2.9809\nValidation - Loss: 3.0810\nBest model updated! Saved as vqa_seq2seq_best.pth\n\nEpoch 7/50 - Training...\nStep [50/300] - Loss: 3.0603\nStep [100/300] - Loss: 2.8354\nStep [150/300] - Loss: 3.6334\nStep [200/300] - Loss: 2.3478\nStep [250/300] - Loss: 3.4388\nStep [300/300] - Loss: 2.8849\nEpoch 7 DONE - Train Loss: 2.9355\nValidation - Loss: 3.0332\nBest model updated! Saved as vqa_seq2seq_best.pth\n\nEpoch 8/50 - Training...\nStep [50/300] - Loss: 2.6098\nStep [100/300] - Loss: 3.0564\nStep [150/300] - Loss: 2.7157\nStep [200/300] - Loss: 2.7277\nStep [250/300] - Loss: 3.0226\nStep [300/300] - Loss: 3.0607\nEpoch 8 DONE - Train Loss: 2.8922\nValidation - Loss: 3.0220\nBest model updated! Saved as vqa_seq2seq_best.pth\n\nEpoch 9/50 - Training...\nStep [50/300] - Loss: 3.0220\nStep [100/300] - Loss: 2.4765\nStep [150/300] - Loss: 3.5277\nStep [200/300] - Loss: 2.6377\nStep [250/300] - Loss: 2.2698\nStep [300/300] - Loss: 3.4006\nEpoch 9 DONE - Train Loss: 2.8534\nValidation - Loss: 3.0097\nBest model updated! Saved as vqa_seq2seq_best.pth\n\nEpoch 10/50 - Training...\nStep [50/300] - Loss: 2.6962\nStep [100/300] - Loss: 3.0400\nStep [150/300] - Loss: 2.9181\nStep [200/300] - Loss: 3.0800\nStep [250/300] - Loss: 2.0115\nStep [300/300] - Loss: 2.4012\nEpoch 10 DONE - Train Loss: 2.8177\nValidation - Loss: 2.9970\nBest model updated! Saved as vqa_seq2seq_best.pth\n\nEpoch 11/50 - Training...\nStep [50/300] - Loss: 2.5989\nStep [100/300] - Loss: 2.9282\nStep [150/300] - Loss: 2.5760\nStep [200/300] - Loss: 3.0462\nStep [250/300] - Loss: 2.9726\nStep [300/300] - Loss: 2.1643\nEpoch 11 DONE - Train Loss: 2.7803\nValidation - Loss: 2.9769\nBest model updated! Saved as vqa_seq2seq_best.pth\n\nEpoch 12/50 - Training...\nStep [50/300] - Loss: 2.9190\nStep [100/300] - Loss: 3.0919\nStep [150/300] - Loss: 2.4176\nStep [200/300] - Loss: 2.3408\nStep [250/300] - Loss: 2.7529\nStep [300/300] - Loss: 2.6984\nEpoch 12 DONE - Train Loss: 2.7505\nValidation - Loss: 2.9723\nBest model updated! Saved as vqa_seq2seq_best.pth\n\nEpoch 13/50 - Training...\nStep [50/300] - Loss: 3.0084\nStep [100/300] - Loss: 2.8380\nStep [150/300] - Loss: 2.2666\nStep [200/300] - Loss: 2.1174\nStep [250/300] - Loss: 2.8156\nStep [300/300] - Loss: 2.9072\nEpoch 13 DONE - Train Loss: 2.7233\nValidation - Loss: 2.9572\nBest model updated! Saved as vqa_seq2seq_best.pth\n\nEpoch 14/50 - Training...\nStep [50/300] - Loss: 2.6184\nStep [100/300] - Loss: 2.4576\nStep [150/300] - Loss: 2.7876\nStep [200/300] - Loss: 3.5312\nStep [250/300] - Loss: 3.3272\nStep [300/300] - Loss: 2.8265\nEpoch 14 DONE - Train Loss: 2.6891\nValidation - Loss: 2.9570\nBest model updated! Saved as vqa_seq2seq_best.pth\n\nEpoch 15/50 - Training...\nStep [50/300] - Loss: 2.3586\nStep [100/300] - Loss: 3.1988\nStep [150/300] - Loss: 2.6613\nStep [200/300] - Loss: 2.8029\nStep [250/300] - Loss: 2.4812\nStep [300/300] - Loss: 2.8819\nEpoch 15 DONE - Train Loss: 2.6584\nValidation - Loss: 2.9614\n\nEpoch 16/50 - Training...\nStep [50/300] - Loss: 2.4047\nStep [100/300] - Loss: 2.6055\nStep [150/300] - Loss: 2.6229\nStep [200/300] - Loss: 3.3633\nStep [250/300] - Loss: 2.1804\nStep [300/300] - Loss: 2.1890\nEpoch 16 DONE - Train Loss: 2.6332\nValidation - Loss: 2.9419\nBest model updated! Saved as vqa_seq2seq_best.pth\n\nEpoch 17/50 - Training...\nStep [50/300] - Loss: 2.7938\nStep [100/300] - Loss: 2.4172\nStep [150/300] - Loss: 2.5362\nStep [200/300] - Loss: 2.3989\nStep [250/300] - Loss: 2.6126\nStep [300/300] - Loss: 2.4191\nEpoch 17 DONE - Train Loss: 2.6042\nValidation - Loss: 2.9389\nBest model updated! Saved as vqa_seq2seq_best.pth\n\nEpoch 18/50 - Training...\nStep [50/300] - Loss: 3.0666\nStep [100/300] - Loss: 2.1656\nStep [150/300] - Loss: 2.2546\nStep [200/300] - Loss: 2.4366\nStep [250/300] - Loss: 2.4340\nStep [300/300] - Loss: 2.7991\nEpoch 18 DONE - Train Loss: 2.5746\nValidation - Loss: 2.9454\n\nEpoch 19/50 - Training...\nStep [50/300] - Loss: 3.2182\nStep [100/300] - Loss: 2.5400\nStep [150/300] - Loss: 2.4704\nStep [200/300] - Loss: 2.2760\nStep [250/300] - Loss: 2.4814\nStep [300/300] - Loss: 2.6561\nEpoch 19 DONE - Train Loss: 2.5376\nValidation - Loss: 2.9439\n\nEpoch 20/50 - Training...\nStep [50/300] - Loss: 2.1936\nStep [100/300] - Loss: 2.2837\nStep [150/300] - Loss: 2.3828\nStep [200/300] - Loss: 2.6738\nStep [250/300] - Loss: 3.2738\nStep [300/300] - Loss: 2.4931\nEpoch 20 DONE - Train Loss: 2.4991\nValidation - Loss: 2.9376\nBest model updated! Saved as vqa_seq2seq_best.pth\n\nEpoch 21/50 - Training...\nStep [50/300] - Loss: 2.7235\nStep [100/300] - Loss: 3.0062\nStep [150/300] - Loss: 2.0943\nStep [200/300] - Loss: 2.3735\nStep [250/300] - Loss: 2.4634\nStep [300/300] - Loss: 2.1305\nEpoch 21 DONE - Train Loss: 2.4821\nValidation - Loss: 2.9505\n\nEpoch 22/50 - Training...\nStep [50/300] - Loss: 2.8457\nStep [100/300] - Loss: 2.2358\nStep [150/300] - Loss: 2.4385\nStep [200/300] - Loss: 2.2086\nStep [250/300] - Loss: 3.1990\nStep [300/300] - Loss: 2.9684\nEpoch 22 DONE - Train Loss: 2.4461\nValidation - Loss: 2.9735\n\nEpoch 23/50 - Training...\nStep [50/300] - Loss: 2.3772\nStep [100/300] - Loss: 2.1245\nStep [150/300] - Loss: 2.1256\nStep [200/300] - Loss: 2.4552\nStep [250/300] - Loss: 2.7134\nStep [300/300] - Loss: 2.9757\nEpoch 23 DONE - Train Loss: 2.4098\nValidation - Loss: 2.9805\n\nEpoch 24/50 - Training...\nStep [50/300] - Loss: 2.0718\nStep [100/300] - Loss: 2.1434\nStep [150/300] - Loss: 2.3679\nStep [200/300] - Loss: 2.1382\nStep [250/300] - Loss: 2.5240\nStep [300/300] - Loss: 3.0343\nEpoch 24 DONE - Train Loss: 2.3804\nValidation - Loss: 2.9825\n\nEpoch 25/50 - Training...\nStep [50/300] - Loss: 2.2929\nStep [100/300] - Loss: 2.6029\nStep [150/300] - Loss: 2.4385\nStep [200/300] - Loss: 2.7469\nStep [250/300] - Loss: 2.3164\nStep [300/300] - Loss: 2.4018\nEpoch 25 DONE - Train Loss: 2.3415\nValidation - Loss: 2.9518\n\nEpoch 26/50 - Training...\nStep [50/300] - Loss: 2.3787\nStep [100/300] - Loss: 2.5099\nStep [150/300] - Loss: 2.4770\nStep [200/300] - Loss: 2.2649\nStep [250/300] - Loss: 2.2722\nStep [300/300] - Loss: 1.8970\nEpoch 26 DONE - Train Loss: 2.3146\nValidation - Loss: 2.9565\n\nEpoch 27/50 - Training...\nStep [50/300] - Loss: 2.1632\nStep [100/300] - Loss: 2.5826\nStep [150/300] - Loss: 2.2545\nStep [200/300] - Loss: 2.0691\nStep [250/300] - Loss: 2.9795\nStep [300/300] - Loss: 2.1122\nEpoch 27 DONE - Train Loss: 2.2681\nValidation - Loss: 2.9592\n\nEpoch 28/50 - Training...\nStep [50/300] - Loss: 1.9837\nStep [100/300] - Loss: 2.0063\nStep [150/300] - Loss: 1.8669\nStep [200/300] - Loss: 2.0480\nStep [250/300] - Loss: 2.1116\nStep [300/300] - Loss: 1.9631\nEpoch 28 DONE - Train Loss: 2.2365\nValidation - Loss: 2.9689\n\nEpoch 29/50 - Training...\nStep [50/300] - Loss: 2.3583\nStep [100/300] - Loss: 2.1604\nStep [150/300] - Loss: 2.1655\nStep [200/300] - Loss: 2.1870\nStep [250/300] - Loss: 2.4958\nStep [300/300] - Loss: 2.6322\nEpoch 29 DONE - Train Loss: 2.2058\nValidation - Loss: 2.9791\n\nEpoch 30/50 - Training...\nStep [50/300] - Loss: 1.8808\nStep [100/300] - Loss: 2.2605\nStep [150/300] - Loss: 2.3137\nStep [200/300] - Loss: 2.4683\nStep [250/300] - Loss: 1.8659\nStep [300/300] - Loss: 2.1644\nEpoch 30 DONE - Train Loss: 2.1658\nValidation - Loss: 2.9912\n\nEpoch 31/50 - Training...\nStep [50/300] - Loss: 2.4307\nStep [100/300] - Loss: 2.4213\nStep [150/300] - Loss: 2.2185\nStep [200/300] - Loss: 2.7633\nStep [250/300] - Loss: 2.1490\nStep [300/300] - Loss: 1.8762\nEpoch 31 DONE - Train Loss: 2.1325\nValidation - Loss: 3.0029\n\nEpoch 32/50 - Training...\nStep [50/300] - Loss: 1.9360\nStep [100/300] - Loss: 1.7229\nStep [150/300] - Loss: 2.1172\nStep [200/300] - Loss: 2.2588\nStep [250/300] - Loss: 2.0368\nStep [300/300] - Loss: 2.3078\nEpoch 32 DONE - Train Loss: 2.1011\nValidation - Loss: 2.9937\n\nEpoch 33/50 - Training...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Kh·ªüi t·∫°o LSTM Encoder with GloVe\nvocab_size = len(word2idx)\nuse_attention = False\nlstm_encoder = QuestionLSTM(vocab_size, pretrained_embeddings=pretrained_embeddings, use_attention=use_attention)\n\n\n# Kh·ªüi t·∫°o VQAModel\nnum_classes = df_vqa[\"label\"].nunique()  # S·ªë l·ªõp c·∫ßn ph√¢n lo·∫°i\nvqa_model = VQAModel(resnet_extractor, lstm_encoder, num_classes=num_classes).to(device)\n\n# # Loss function & Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(vqa_model.parameters(), lr=1e-4, weight_decay=1e-5)  # Th√™m weight decay ƒë·ªÉ tr√°nh overfitting\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:57:53.398146Z","iopub.status.idle":"2025-03-26T02:57:53.398398Z","shell.execute_reply":"2025-03-26T02:57:53.398294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Danh s√°ch l∆∞u l·ªãch s·ª≠ loss\ntrain_losses = []\nval_losses = []\n\nnum_epochs = 50\nbest_val_acc = 0  # Bi·∫øn theo d√µi ƒë·ªô ch√≠nh x√°c t·ªët nh·∫•t\n\nfor epoch in range(num_epochs):\n    # ----- TRAINING -----\n    vqa_model.train()\n    total_loss, correct, total = 0, 0, 0\n\n    print(f\"\\nEpoch {epoch+1}/{num_epochs} - Training...\")\n\n    for batch_idx, (image_ids, images, questions, labels) in enumerate(train_loader):\n        images, questions, labels = images.to(device), questions.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = vqa_model(images, questions)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        preds = outputs.argmax(dim=1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n        # Hi·ªÉn th·ªã sau m·ªói 50 step\n        if (batch_idx + 1) % 50 == 0 or batch_idx == len(train_loader) - 1:\n            train_acc = 100 * correct / total\n            print(f\"Step [{batch_idx+1}/{len(train_loader)}] - Loss: {loss.item():.4f}, Train Acc: {train_acc:.2f}%\")\n\n    train_loss = total_loss / len(train_loader)\n    train_acc = 100 * correct / total\n    print(f\"Epoch {epoch+1} DONE - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n    \n    # L∆∞u train loss v√†o l·ªãch s·ª≠\n    train_losses.append(train_loss)\n\n    # ----- VALIDATION -----\n    vqa_model.eval()\n    val_loss, val_correct, val_total = 0, 0, 0\n    num_batches = 0  # üî• ƒê·∫øm s·ªë batch ƒë·ªÉ tr√°nh chia cho 0\n\n    with torch.no_grad():  # Kh√¥ng t√≠nh gradient khi validation\n        for batch_idx, (image_ids, images, questions, labels) in enumerate(val_loader):\n            images, questions, labels = images.to(device), questions.to(device), labels.to(device)\n\n            outputs = vqa_model(images, questions)\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item()\n            preds = outputs.argmax(dim=1)\n            val_correct += (preds == labels).sum().item()\n            val_total += labels.size(0)\n            num_batches += 1\n\n    # Ki·ªÉm tra n·∫øu c√≥ batch validation, tr√°nh chia cho 0\n    if num_batches > 0:\n        avg_val_loss = val_loss / num_batches\n        val_acc = 100.0 * val_correct / val_total\n    else:\n        avg_val_loss = float('inf')  # N·∫øu kh√¥ng c√≥ batch n√†o, ƒë·∫∑t loss cao ƒë·ªÉ tr√°nh overfitting\n        val_acc = 0.0\n\n    print(f\"Validation - Loss: {avg_val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n\n    # L∆∞u validation loss v√†o l·ªãch s·ª≠\n    val_losses.append(avg_val_loss)\n\n    # ----- L∆∞u model t·ªët nh·∫•t d·ª±a tr√™n val_acc v√† use_attention -----\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        # Ch·ªçn t√™n file l∆∞u d·ª±a tr√™n gi√° tr·ªã c·ªßa `use_attention`\n        model_filename = \"vqa_pretrained_with_attn.pth\" if use_attention else \"vqa_pretrained_no_attn.pth\"\n\n        # L∆∞u model\n        torch.save(vqa_model.state_dict(), model_filename)\n        print(f\"Best model updated! Saved as {model_filename}\")\n\n# V·∫Ω bi·ªÉu ƒë·ªì loss\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\nplt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T02:57:53.399271Z","iopub.status.idle":"2025-03-26T02:57:53.399552Z","shell.execute_reply":"2025-03-26T02:57:53.399447Z"}},"outputs":[],"execution_count":null}]}